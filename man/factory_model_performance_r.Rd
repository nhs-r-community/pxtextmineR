% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/factory_model_performance_r.R
\name{factory_model_performance_r}
\alias{factory_model_performance_r}
\title{Evaluate the performance of a fitted pipeline}
\usage{
factory_model_performance_r(pipe, x_train, y_train, x_test, y_test, metric)
}
\arguments{
\item{x_train}{A data frame. Training data (predictor).}

\item{y_train}{A vector. Training data (response).}

\item{x_test}{A data frame. Test data (predictor).}

\item{y_test}{A vector. Test data (response).}

\item{metric}{A string. Scorer that was used in pipeline tuning
("accuracy_score", "balanced_accuracy_score", "matthews_corrcoef",
"class_balance_accuracy_score")}
}
\value{
A list of length 5:
\itemize{
\item{\code{pipe} The fitted \code{Scikit-learn}/\code{imblearn} pipeline.}
\item{\code{tuning_results} A data frame with all (hyper)parameter values
and models tried during fitting.
}
\item{\code{pred} A vector with the predictions on the test
set.
}
\item{\code{accuracy_per_class} A data frame with accuracies per class.}
\item{\code{p_compare_models_bar} A bar plot comparing the mean scores (of
the user-supplied \code{metric} parameter) from the cross-validation
on the training set, for the best (hyper)parameter values for
each learner.
}
}
}
\description{
Performance metrics on the test set.
}
\note{
Returned object \code{tuning_results} lists all (hyper)parameter values
tried during pipeline fitting, along with performance metrics. It is
generated from the \code{Scikit-learn} output that follows pipeline fitting.
It is derived from attribute \href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html}{\code{cv_results_}}
with some modifications. In R, \code{cv_results_} can be accessed following
fitting of a pipeline with \code{pxtextmineR::factory_pipeline_r} or by
calling function \code{pxtextmineR::factory_model_performance_r}. Say that
the fitted pipeline is assigned to an object called \code{pipe}, and that the
pipeline performance is assigned to an object called \code{pipe_performance}.
Then, \code{cv_results_} can be accessed with \code{pipe$cv_results_} or
\code{pipe_performance$cv_results_}. \cr\cr
NOTE: After calculating performance metrics on the test set,
\code{pxtextmineR::factory_model_performance_r} fits the pipeline on the whole
dataset (train + test). Hence, do not be surprised that the pipeline's
\code{score()} method will now return a dramatically improved score on the
test set- it is just a result of overfitting, because the refitted
pipeline has now "seen" the test dataset. See Examples.
}
\examples{
# Prepare training and test sets
data_splits <- pxtextmineR::factory_data_load_and_split_r(
  filename = pxtextmineR::text_data,
  target = "label",
  predictor = "feedback",
  test_size = 0.90) # Make a small training set for a faster run in this example

# Let's take a look at the returned list
str(data_splits)

# Fit the pipeline
pipe <- pxtextmineR::factory_pipeline_r(
  x = data_splits$x_train,
  y = data_splits$y_train,
  tknz = "spacy",
  ordinal = FALSE,
  metric = "accuracy_score",
  cv = 2, n_iter = 10, n_jobs = 1, verbose = 3,
  learners = c("SGDClassifier", "MultinomialNB")
)

# Assess model performance
pipe_performance <- pxtextmineR::factory_model_performance_r(
  pipe = pipe,
  x_train = data_splits$x_train,
  y_train = data_splits$y_train,
  x_test = data_splits$x_test,
  y_test = data_splits$y_test,
  metric = "accuracy_score")

names(pipe_performance)

# Let's compare pipeline performance for different tunings with a range of metrics
pipe_performance$
  tuning_results \%>\%
  dplyr::select(learner, dplyr::contains("mean_test"))

# A glance at the (hyper)parameters and their tuned values
pipe_performance$
  tuning_results \%>\%
  dplyr::select(learner, dplyr::contains("param_")) \%>\%
  str()

# Accuracy per class
pipe_performance$accuracy_per_class

# Learner performance barplot
pipe_performance$p_compare_models_bar

# Predictions on test set
preds <- pipe_performance$pred
head(preds)

################################################################################
# NOTE!!! #
################################################################################
# After calculating performance metrics on the test set,
# pxtextmineR::factory_model_performance_r fits the pipeline on the whole
# dataset (train + test). Hence, do not be surprised that the pipeline's score()
# method will now return a dramatically improved score on the test set- it is
# just a result of overfitting, because the refitted pipeline has now "seen" the
# test dataset.
pipe_performance$pipe$score(data_splits$x_test, data_splits$y_test)
pipe$score(data_splits$x_test, data_splits$y_test)

# We can confirm this score by having the re-fitted pipeline predict x_test
# again. The predictions will be better and new accuracy score will be the
# inflated one.
preds_refitted <- pipe$predict(data_splits$x_test)

score_refitted <- data_splits$y_test \%>\%
  data.frame() \%>\%
  dplyr::rename(true = '.') \%>\%
  dplyr::mutate(
    pred = preds_refitted,
    check = true == preds_refitted,
    check = sum(check) / nrow(.)
  ) \%>\%
  dplyr::pull(check) \%>\%
  unique()

  score_refitted

# Compare this to the ACTUAL performance on the test dataset
preds_actual <- pipe_performance$pred

score_actual <- data_splits$y_test \%>\%
  data.frame() \%>\%
  dplyr::rename(true = '.') \%>\%
  dplyr::mutate(
    pred = preds_actual,
    check = true == preds_actual,
    check = sum(check) / nrow(.)
  ) \%>\%
  dplyr::pull(check) \%>\%
  unique()

  score_actual

  score_refitted - score_actual
}
\references{
Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O.,
Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A.,
Cournapeau D., Brucher M., Perrot M. & Duchesnay E. (2011),
\href{https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html}{Scikit-learn: Machine Learning in Python}.
\emph{Journal of Machine Learning Research} 12:2825â€“-2830.
}
